{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f522a01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install nb_black==1.0.7 nltk==3.6.6 jupyterlab_pygments==0.1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccc04483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "black @ file:///home/conda/feedstock_root/build_artifacts/black-recipe_1599478779128/work\n",
      "boto3==1.20.25\n",
      "hdijupyterutils==0.19.1\n",
      "jupyter @ file:///home/conda/feedstock_root/build_artifacts/jupyter_1611871900595/work\n",
      "jupyter-client @ file:///home/conda/feedstock_root/build_artifacts/jupyter_client_1610375432619/work\n",
      "jupyter-console==6.4.0\n",
      "jupyter-core @ file:///home/conda/feedstock_root/build_artifacts/jupyter_core_1612125253553/work\n",
      "jupyter-packaging @ file:///home/conda/feedstock_root/build_artifacts/jupyter-packaging_1613054948399/work\n",
      "jupyter-server==1.13.0\n",
      "jupyterlab==3.2.4\n",
      "jupyterlab-launcher==0.13.1\n",
      "jupyterlab-pygments @ file:///home/conda/feedstock_root/build_artifacts/jupyterlab_pygments_1601375948261/work\n",
      "jupyterlab-server @ file:///home/conda/feedstock_root/build_artifacts/jupyterlab_server_1613760084674/work\n",
      "jupyterlab-widgets @ file:///home/conda/feedstock_root/build_artifacts/jupyterlab_widgets_1609173350931/work\n",
      "nb-black==1.0.7\n",
      "nltk==3.6.6\n",
      "pandas==1.1.5\n",
      "pyls-black @ file:///home/conda/feedstock_root/build_artifacts/pyls-black_1595615126037/work\n",
      "pyspark==2.4.0\n",
      "sagemaker==2.72.1\n",
      "sagemaker-pyspark==1.4.2\n",
      "CPU times: user 11.7 ms, sys: 12.6 ms, total: 24.3 ms\n",
      "Wall time: 899 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!pip freeze | grep -E 'boto3|black|sagemaker|sagemaker_pyspark|jupyter|jupyterlab|pandas|pyspark|nltk'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f670e2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext lab_black\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02a34ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List\n",
    "\n",
    "import boto3\n",
    "import nltk\n",
    "import sagemaker_pyspark\n",
    "from pyspark import SparkConf, keyword_only\n",
    "from pyspark.ml import Pipeline, Transformer\n",
    "from pyspark.ml.clustering import LDA\n",
    "from pyspark.ml.feature import (\n",
    "    CountVectorizer,\n",
    "    IDF,\n",
    "    RegexTokenizer,\n",
    "    StopWordsRemover,\n",
    "    Tokenizer,\n",
    ")\n",
    "from pyspark.ml.param.shared import (\n",
    "    HasInputCol,\n",
    "    HasOutputCol,\n",
    "    Param,\n",
    "    Params,\n",
    "    TypeConverters,\n",
    ")\n",
    "from pyspark.ml.linalg import SparseVector, Vectors\n",
    "from pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable\n",
    "from pyspark.sql import Column, SparkSession, functions as F, types as T\n",
    "from pyspark.sql.dataframe import DataFrame as pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73841ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # not used\n",
    "# import dask.dataframe as dd\n",
    "# import pandas as pd\n",
    "\n",
    "# used for display purposes only\n",
    "from pandas import DataFrame as pd_DataFrame, option_context as pd_option_context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54369980",
   "metadata": {},
   "source": [
    "## User Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12ed4ffa",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# S3\n",
    "s3_bucket_name = \"sagemakertestwillz3s\"\n",
    "path_to_folder = \"/datasets/twitter/kinesis-demo/\"\n",
    "\n",
    "# Data processing\n",
    "all_cols_to_process = [\"created_at\", \"user_joined\", \"reviewText\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1afa4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_pyspark_df(df: pdf, nrows: int = 5) -> pd_DataFrame:\n",
    "    \"\"\"Display the first n rows of a PySpark DataFrame as a Pandas DataFrame.\"\"\"\n",
    "    return df.limit(nrows).toPandas()\n",
    "\n",
    "\n",
    "def get_existing_csv_files_list(s3_bucket_name: str, prefix: str) -> List[str]:\n",
    "    \"\"\"Get list of files in subfolder in S3 bucket.\"\"\"\n",
    "    s3_resource = boto3.resource(\"s3\")\n",
    "    bucket = s3_resource.Bucket(s3_bucket_name)\n",
    "    files_found_objects_list = list(bucket.objects.filter(Prefix=prefix))\n",
    "    files_found_names_list = [w.key for w in files_found_objects_list]\n",
    "    return files_found_names_list\n",
    "\n",
    "\n",
    "def remove_punctuation(column_obj: Column) -> Column:\n",
    "    \"\"\"Removes punctuation from a DataFrame text column.\"\"\"\n",
    "    return F.regexp_replace(column_obj, \"\\\\p{Punct}\", \"\")\n",
    "\n",
    "\n",
    "def remove_lead_trail_spaces(column_obj: Column) -> Column:\n",
    "    \"\"\"Removes leading and trailing spaces from a DataFrame text column.\"\"\"\n",
    "    return F.trim(column_obj)\n",
    "\n",
    "\n",
    "def replace_multiple_spaces(column_obj: Column) -> Column:\n",
    "    \"\"\"Replace multiple spaces with a single space.\"\"\"\n",
    "    return F.regexp_replace(column_obj, r\"\\s+\", \" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4198610b",
   "metadata": {},
   "source": [
    "Download NLTK stopwords, if not previously done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e2954a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.46 ms, sys: 0 ns, total: 1.46 ms\n",
      "Wall time: 1.23 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if not os.path.isdir(\n",
    "    os.path.join(os.path.expanduser(\"~\"), \"nltk_data\", \"corpora\", \"stopwords\")\n",
    "):\n",
    "    nltk.download(\"stopwords\")\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "all_stopwords = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b34ae3",
   "metadata": {},
   "source": [
    "## PySpark Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5cac37f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 674 µs, sys: 139 µs, total: 813 µs\n",
      "Wall time: 809 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "conf = (SparkConf()\n",
    "        .set(\"spark.driver.extraClassPath\", \":\".join(sagemaker_pyspark.classpath_jars())))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb89b4c",
   "metadata": {},
   "source": [
    "Start a Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3180556a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21.1 ms, sys: 5.39 ms, total: 26.5 ms\n",
      "Wall time: 3.62 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .config(conf=conf) \\\n",
    "    .appName(\"schema_test\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04db9f6f",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbb4754",
   "metadata": {},
   "source": [
    "### Get List of S3 CSV Data Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34689e0",
   "metadata": {},
   "source": [
    "Get a list of all the CSV files containing the tweets data (files with a prefix `tweets_*.csv`), and not the metadata (prefix `tweets_metadata_*.csv`), from `csvs/` folder in the S3 bucket path at `<bucket-name>/datasets/twitter/kinesis-demo/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b55a4974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 82 ms, sys: 15.6 ms, total: 97.5 ms\n",
      "Wall time: 186 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['datasets/twitter/kinesis-demo/csvs/tweets_15_hc2021123017_s20211230133054.csv']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "existing_csv_files_list = get_existing_csv_files_list(\n",
    "    s3_bucket_name, path_to_folder[1:] + \"csvs/tweets_\"\n",
    ")\n",
    "files_csvs_list = [f for f in existing_csv_files_list if \"metadata\" not in f]\n",
    "files_csvs_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6f87d3",
   "metadata": {},
   "source": [
    "### Load all CSV Files into Single PySpark `DataFrame`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278c7fa5",
   "metadata": {},
   "source": [
    "Read all CSV files from the `csvs/` in the S3 bucket path at `<bucket-name>/datasets/twitter/kinesis-demo/` into a PySpark `DataFrame`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f96b03b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.95 ms, sys: 598 µs, total: 3.55 ms\n",
      "Wall time: 5.41 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = spark.read.csv(\n",
    "    [f's3a://{s3_bucket_name}' + f\"/{f}\" for f in files_csvs_list],\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ").withColumnRenamed(\"text\", \"reviewText\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff67afd",
   "metadata": {},
   "source": [
    "Get the number of rows (retrieved tweets) in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4093f31e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data contains 2,055 rows and 54 columns\n"
     ]
    }
   ],
   "source": [
    "print(f\"Raw data contains {df.count():,} rows and {len(df.columns):,} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b65e02",
   "metadata": {},
   "source": [
    "Show the first 4 rows from the PySpark `DataFrame`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a98e4c5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>geo</th>\n",
       "      <th>coordinates</th>\n",
       "      <th>place</th>\n",
       "      <th>contributors</th>\n",
       "      <th>is_quote_status</th>\n",
       "      <th>quote_count</th>\n",
       "      <th>reply_count</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>favorited</th>\n",
       "      <th>retweeted</th>\n",
       "      <th>created_at</th>\n",
       "      <th>source</th>\n",
       "      <th>in_reply_to_user_id</th>\n",
       "      <th>in_reply_to_screen_name</th>\n",
       "      <th>source_text</th>\n",
       "      <th>place_id</th>\n",
       "      <th>place_url</th>\n",
       "      <th>place_place_type</th>\n",
       "      <th>place_name</th>\n",
       "      <th>place_full_name</th>\n",
       "      <th>place_country_code</th>\n",
       "      <th>place_country</th>\n",
       "      <th>place_bounding_box_type</th>\n",
       "      <th>place_bounding_box_coordinates</th>\n",
       "      <th>place_attributes</th>\n",
       "      <th>coords_type</th>\n",
       "      <th>coords_lon</th>\n",
       "      <th>coords_lat</th>\n",
       "      <th>geo_type</th>\n",
       "      <th>geo_lon</th>\n",
       "      <th>geo_lat</th>\n",
       "      <th>user_name</th>\n",
       "      <th>user_screen_name</th>\n",
       "      <th>user_followers</th>\n",
       "      <th>user_friends</th>\n",
       "      <th>user_listed</th>\n",
       "      <th>user_favourites</th>\n",
       "      <th>user_statuses</th>\n",
       "      <th>user_protected</th>\n",
       "      <th>user_verified</th>\n",
       "      <th>user_contributors_enabled</th>\n",
       "      <th>user_joined</th>\n",
       "      <th>user_location</th>\n",
       "      <th>retweeted_tweet</th>\n",
       "      <th>tweet_text_urls</th>\n",
       "      <th>tweet_text_hashtags</th>\n",
       "      <th>tweet_text_usernames</th>\n",
       "      <th>num_urls_in_tweet_text</th>\n",
       "      <th>num_users_in_tweet_text</th>\n",
       "      <th>num_hashtags_in_tweet_text</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>file_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1476607990027407364</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2021-12-30 17:35:53+00:00</td>\n",
       "      <td>\"&lt;a href=\"\"http://twitter.com/download/iphone\"...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[[]]</td>\n",
       "      <td>{}</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Happy New Years 2022</td>\n",
       "      <td>GuessWho122021</td>\n",
       "      <td>216</td>\n",
       "      <td>255</td>\n",
       "      <td>0</td>\n",
       "      <td>1165</td>\n",
       "      <td>3214</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2021-11-27 20:38:14+00:00</td>\n",
       "      <td>World Wide</td>\n",
       "      <td>no</td>\n",
       "      <td>https://t.co/yH8cBVuMj3</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Space colonists may turn to cannibalism, scien...</td>\n",
       "      <td>twitter_delivery_stream-1-2021-12-30-17-35-58-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1476607992586018824</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2021-12-30 17:35:54+00:00</td>\n",
       "      <td>\"&lt;a href=\"\"http://twitter.com/download/iphone\"...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[[]]</td>\n",
       "      <td>{}</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>TheToysRusKid</td>\n",
       "      <td>JC1of1</td>\n",
       "      <td>202</td>\n",
       "      <td>313</td>\n",
       "      <td>0</td>\n",
       "      <td>18454</td>\n",
       "      <td>53653</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2018-09-04 17:18:46+00:00</td>\n",
       "      <td>Southern California</td>\n",
       "      <td>no</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1. Beat Street2. Harlem Knights 3. National La...</td>\n",
       "      <td>twitter_delivery_stream-1-2021-12-30-17-35-58-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1476607997405184000</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2021-12-30 17:35:55+00:00</td>\n",
       "      <td>\"&lt;a href=\"\"http://publicize.wp.com/\"\" rel=\"\"no...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>WordPress.com</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[[]]</td>\n",
       "      <td>{}</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>BCABA Network</td>\n",
       "      <td>BcabaNetwork</td>\n",
       "      <td>2581</td>\n",
       "      <td>4710</td>\n",
       "      <td>42</td>\n",
       "      <td>3027</td>\n",
       "      <td>238152</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2016-05-25 10:54:46+00:00</td>\n",
       "      <td>West Midlands, England</td>\n",
       "      <td>no</td>\n",
       "      <td>https://t.co/kPAMK3TxSL</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NASA Plans Coverage of Webb Space Telescope De...</td>\n",
       "      <td>twitter_delivery_stream-1-2021-12-30-17-35-58-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1476607997925330945</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2021-12-30 17:35:55+00:00</td>\n",
       "      <td>\"&lt;a href=\"\"http://twitter.com/download/iphone\"...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[[]]</td>\n",
       "      <td>{}</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Jude Jackson 💙 Solidarity with #NHS workers</td>\n",
       "      <td>JudeJack</td>\n",
       "      <td>2138</td>\n",
       "      <td>4999</td>\n",
       "      <td>157</td>\n",
       "      <td>79973</td>\n",
       "      <td>109065</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2009-02-21 14:48:52+00:00</td>\n",
       "      <td>International</td>\n",
       "      <td>no</td>\n",
       "      <td>https://t.co/cRcHiFCHS0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Exciting times!Back in 1915, Einstein publishe...</td>\n",
       "      <td>twitter_delivery_stream-1-2021-12-30-17-35-58-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id   geo coordinates place contributors  is_quote_status  \\\n",
       "0  1476607990027407364  None        None  None         None            False   \n",
       "1  1476607992586018824  None        None  None         None             True   \n",
       "2  1476607997405184000  None        None  None         None            False   \n",
       "3  1476607997925330945  None        None  None         None            False   \n",
       "\n",
       "   quote_count  reply_count  retweet_count  favorite_count  favorited  \\\n",
       "0            0            0              0               0      False   \n",
       "1            0            0              0               0      False   \n",
       "2            0            0              0               0      False   \n",
       "3            0            0              0               0      False   \n",
       "\n",
       "   retweeted                 created_at  \\\n",
       "0      False  2021-12-30 17:35:53+00:00   \n",
       "1      False  2021-12-30 17:35:54+00:00   \n",
       "2      False  2021-12-30 17:35:55+00:00   \n",
       "3      False  2021-12-30 17:35:55+00:00   \n",
       "\n",
       "                                              source in_reply_to_user_id  \\\n",
       "0  \"<a href=\"\"http://twitter.com/download/iphone\"...                None   \n",
       "1  \"<a href=\"\"http://twitter.com/download/iphone\"...                None   \n",
       "2  \"<a href=\"\"http://publicize.wp.com/\"\" rel=\"\"no...                None   \n",
       "3  \"<a href=\"\"http://twitter.com/download/iphone\"...                None   \n",
       "\n",
       "  in_reply_to_screen_name         source_text place_id place_url  \\\n",
       "0                    None  Twitter for iPhone     None      None   \n",
       "1                    None  Twitter for iPhone     None      None   \n",
       "2                    None       WordPress.com     None      None   \n",
       "3                    None  Twitter for iPhone     None      None   \n",
       "\n",
       "  place_place_type place_name place_full_name place_country_code  \\\n",
       "0             None       None            None               None   \n",
       "1             None       None            None               None   \n",
       "2             None       None            None               None   \n",
       "3             None       None            None               None   \n",
       "\n",
       "  place_country place_bounding_box_type place_bounding_box_coordinates  \\\n",
       "0          None                    None                           [[]]   \n",
       "1          None                    None                           [[]]   \n",
       "2          None                    None                           [[]]   \n",
       "3          None                    None                           [[]]   \n",
       "\n",
       "  place_attributes coords_type coords_lon coords_lat geo_type geo_lon geo_lat  \\\n",
       "0               {}        None       None       None     None    None    None   \n",
       "1               {}        None       None       None     None    None    None   \n",
       "2               {}        None       None       None     None    None    None   \n",
       "3               {}        None       None       None     None    None    None   \n",
       "\n",
       "                                     user_name user_screen_name  \\\n",
       "0                         Happy New Years 2022   GuessWho122021   \n",
       "1                                TheToysRusKid           JC1of1   \n",
       "2                                BCABA Network     BcabaNetwork   \n",
       "3  Jude Jackson 💙 Solidarity with #NHS workers         JudeJack   \n",
       "\n",
       "  user_followers  user_friends  user_listed  user_favourites  user_statuses  \\\n",
       "0            216           255            0             1165           3214   \n",
       "1            202           313            0            18454          53653   \n",
       "2           2581          4710           42             3027         238152   \n",
       "3           2138          4999          157            79973         109065   \n",
       "\n",
       "  user_protected  user_verified  user_contributors_enabled  \\\n",
       "0          False          False                      False   \n",
       "1          False          False                      False   \n",
       "2          False          False                      False   \n",
       "3          False          False                      False   \n",
       "\n",
       "                 user_joined           user_location retweeted_tweet  \\\n",
       "0  2021-11-27 20:38:14+00:00              World Wide              no   \n",
       "1  2018-09-04 17:18:46+00:00    Southern California               no   \n",
       "2  2016-05-25 10:54:46+00:00  West Midlands, England              no   \n",
       "3  2009-02-21 14:48:52+00:00           International              no   \n",
       "\n",
       "           tweet_text_urls tweet_text_hashtags tweet_text_usernames  \\\n",
       "0  https://t.co/yH8cBVuMj3                None                 None   \n",
       "1                     None                None                 None   \n",
       "2  https://t.co/kPAMK3TxSL                None                 None   \n",
       "3  https://t.co/cRcHiFCHS0                None                 None   \n",
       "\n",
       "   num_urls_in_tweet_text  num_users_in_tweet_text  \\\n",
       "0                       1                        0   \n",
       "1                       0                        0   \n",
       "2                       1                        0   \n",
       "3                       1                        0   \n",
       "\n",
       "   num_hashtags_in_tweet_text  \\\n",
       "0                           0   \n",
       "1                           0   \n",
       "2                           0   \n",
       "3                           0   \n",
       "\n",
       "                                          reviewText  \\\n",
       "0  Space colonists may turn to cannibalism, scien...   \n",
       "1  1. Beat Street2. Harlem Knights 3. National La...   \n",
       "2  NASA Plans Coverage of Webb Space Telescope De...   \n",
       "3  Exciting times!Back in 1915, Einstein publishe...   \n",
       "\n",
       "                                           file_name  \n",
       "0  twitter_delivery_stream-1-2021-12-30-17-35-58-...  \n",
       "1  twitter_delivery_stream-1-2021-12-30-17-35-58-...  \n",
       "2  twitter_delivery_stream-1-2021-12-30-17-35-58-...  \n",
       "3  twitter_delivery_stream-1-2021-12-30-17-35-58-...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with pd_option_context(\"display.max_columns\", 100):\n",
    "    display(show_pyspark_df(df, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d984a6",
   "metadata": {},
   "source": [
    "Get the schema for the PySpark `DataFrame`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "de703f9e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- geo: string (nullable = true)\n",
      " |-- coordinates: string (nullable = true)\n",
      " |-- place: string (nullable = true)\n",
      " |-- contributors: string (nullable = true)\n",
      " |-- is_quote_status: boolean (nullable = true)\n",
      " |-- quote_count: integer (nullable = true)\n",
      " |-- reply_count: integer (nullable = true)\n",
      " |-- retweet_count: integer (nullable = true)\n",
      " |-- favorite_count: integer (nullable = true)\n",
      " |-- favorited: boolean (nullable = true)\n",
      " |-- retweeted: boolean (nullable = true)\n",
      " |-- created_at: string (nullable = true)\n",
      " |-- source: string (nullable = true)\n",
      " |-- in_reply_to_user_id: string (nullable = true)\n",
      " |-- in_reply_to_screen_name: string (nullable = true)\n",
      " |-- source_text: string (nullable = true)\n",
      " |-- place_id: string (nullable = true)\n",
      " |-- place_url: string (nullable = true)\n",
      " |-- place_place_type: string (nullable = true)\n",
      " |-- place_name: string (nullable = true)\n",
      " |-- place_full_name: string (nullable = true)\n",
      " |-- place_country_code: string (nullable = true)\n",
      " |-- place_country: string (nullable = true)\n",
      " |-- place_bounding_box_type: string (nullable = true)\n",
      " |-- place_bounding_box_coordinates: string (nullable = true)\n",
      " |-- place_attributes: string (nullable = true)\n",
      " |-- coords_type: string (nullable = true)\n",
      " |-- coords_lon: string (nullable = true)\n",
      " |-- coords_lat: string (nullable = true)\n",
      " |-- geo_type: string (nullable = true)\n",
      " |-- geo_lon: string (nullable = true)\n",
      " |-- geo_lat: string (nullable = true)\n",
      " |-- user_name: string (nullable = true)\n",
      " |-- user_screen_name: string (nullable = true)\n",
      " |-- user_followers: string (nullable = true)\n",
      " |-- user_friends: integer (nullable = true)\n",
      " |-- user_listed: integer (nullable = true)\n",
      " |-- user_favourites: integer (nullable = true)\n",
      " |-- user_statuses: integer (nullable = true)\n",
      " |-- user_protected: string (nullable = true)\n",
      " |-- user_verified: boolean (nullable = true)\n",
      " |-- user_contributors_enabled: boolean (nullable = true)\n",
      " |-- user_joined: string (nullable = true)\n",
      " |-- user_location: string (nullable = true)\n",
      " |-- retweeted_tweet: string (nullable = true)\n",
      " |-- tweet_text_urls: string (nullable = true)\n",
      " |-- tweet_text_hashtags: string (nullable = true)\n",
      " |-- tweet_text_usernames: string (nullable = true)\n",
      " |-- num_urls_in_tweet_text: integer (nullable = true)\n",
      " |-- num_users_in_tweet_text: integer (nullable = true)\n",
      " |-- num_hashtags_in_tweet_text: integer (nullable = true)\n",
      " |-- reviewText: string (nullable = true)\n",
      " |-- file_name: string (nullable = true)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# print(df.printSchema())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0052556",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161b8d4e",
   "metadata": {},
   "source": [
    "For processing the data, text and non-text columns will be treated separately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4b7c1a",
   "metadata": {},
   "source": [
    "### Processing Non-Text Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01088a9",
   "metadata": {},
   "source": [
    "We'll define a PySparkML pipeline ([v2.4.0](https://spark.apache.org/docs/2.4.0/ml-pipeline.html#pipeline), [latest version](https://spark.apache.org/docs/latest/ml-pipeline.html), [API for latest version](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.Pipeline.html)) to process all useful non-text columns from the data. This pipeline will accept a list of all useful columns and return a DataFrame with the same input columns and the processed versions (a suffix will be added to the column name to indicate that it is has been processed and keep it separate from the raw data). As an example, the column `created_at` will be converted from a string into a datetime datatype, and the converted version of this column will be returned as `created_at_dt`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6035dd7",
   "metadata": {},
   "source": [
    "The following processing steps will be applied\n",
    "- drop rows with duplicated tweets (retweets) since the text of the tweet (which will be used in NLP) is repeated\n",
    "  - for analysing text data, we don't need multiple observations (rows) with the same text (re-tweets)\n",
    "    - such rows are only useful when exploring the data after the text-based analysis has been completed\n",
    "  - so, for NLP analysis, the duplicated (re-tweets) rows in the text column can be dropped\n",
    "  - based on how data was collected using `twitter_s3.py`, an example of a retweet is\n",
    "    - (row 10) `retweeted_tweet = 'no'` and text column = `'text here'`\n",
    "    - (row 91) `retweeted_tweet = 'yes'` and text column = `'text here'`\n",
    "    - (row 201) `retweeted_tweet = 'yes'` and text column = `'text here'`\n",
    "\n",
    "    where we only need the first tweet (row 10) and so we can drop all rows corresponding to retweets\n",
    "- drop rows where the tweet is missing or contains an empty string (if any)\n",
    "- convert the following columns from the `string` datatype to `datetime`s\n",
    "  - `created_at` (date and time when the tweet was posted)\n",
    "  - `user_joined` (date and time when user joined Twitter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b806817b",
   "metadata": {},
   "source": [
    "Apply the non-text processing pipeline to process all the useful non-text columns from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "12ee7196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1081\n",
      "CPU times: user 12.8 ms, sys: 4.24 ms, total: 17.1 ms\n",
      "Wall time: 2.39 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>user_joined</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>created_at_dt</th>\n",
       "      <th>user_joined_dt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-12-30 17:41:12+00:00</td>\n",
       "      <td>2021-08-21 13:39:34+00:00</td>\n",
       "      <td>I love you NASA</td>\n",
       "      <td>2021-12-30 17:41:12</td>\n",
       "      <td>2021-08-21 13:39:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-12-30 17:48:32+00:00</td>\n",
       "      <td>2019-03-20 14:31:32+00:00</td>\n",
       "      <td>The X on my back is so large NASA can see it...</td>\n",
       "      <td>2021-12-30 17:48:32</td>\n",
       "      <td>2019-03-20 14:31:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-12-30 17:47:42+00:00</td>\n",
       "      <td>2015-04-29 21:00:18+00:00</td>\n",
       "      <td>Moon and Uranus !</td>\n",
       "      <td>2021-12-30 17:47:42</td>\n",
       "      <td>2015-04-29 21:00:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-12-30 17:43:37+00:00</td>\n",
       "      <td>2009-07-10 15:05:09+00:00</td>\n",
       "      <td>\"  Church people be like \"\"we all know who cre...</td>\n",
       "      <td>2021-12-30 17:43:37</td>\n",
       "      <td>2009-07-10 15:05:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-12-30 17:40:18+00:00</td>\n",
       "      <td>2021-02-23 00:52:38+00:00</td>\n",
       "      <td>Blue Origin is looking to hire a Work Center S...</td>\n",
       "      <td>2021-12-30 17:40:18</td>\n",
       "      <td>2021-02-23 00:52:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2021-12-30 17:38:10+00:00</td>\n",
       "      <td>2021-06-05 07:13:05+00:00</td>\n",
       "      <td>I support this project good luck</td>\n",
       "      <td>2021-12-30 17:38:10</td>\n",
       "      <td>2021-06-05 07:13:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2021-12-30 17:41:02+00:00</td>\n",
       "      <td>2009-03-22 15:14:18+00:00</td>\n",
       "      <td>This account is worth a follow</td>\n",
       "      <td>2021-12-30 17:41:02</td>\n",
       "      <td>2009-03-22 15:14:18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  created_at                user_joined  \\\n",
       "0  2021-12-30 17:41:12+00:00  2021-08-21 13:39:34+00:00   \n",
       "1  2021-12-30 17:48:32+00:00  2019-03-20 14:31:32+00:00   \n",
       "2  2021-12-30 17:47:42+00:00  2015-04-29 21:00:18+00:00   \n",
       "3  2021-12-30 17:43:37+00:00  2009-07-10 15:05:09+00:00   \n",
       "4  2021-12-30 17:40:18+00:00  2021-02-23 00:52:38+00:00   \n",
       "5  2021-12-30 17:38:10+00:00  2021-06-05 07:13:05+00:00   \n",
       "6  2021-12-30 17:41:02+00:00  2009-03-22 15:14:18+00:00   \n",
       "\n",
       "                                          reviewText       created_at_dt  \\\n",
       "0                                    I love you NASA 2021-12-30 17:41:12   \n",
       "1    The X on my back is so large NASA can see it... 2021-12-30 17:48:32   \n",
       "2                                  Moon and Uranus ! 2021-12-30 17:47:42   \n",
       "3  \"  Church people be like \"\"we all know who cre... 2021-12-30 17:43:37   \n",
       "4  Blue Origin is looking to hire a Work Center S... 2021-12-30 17:40:18   \n",
       "5                   I support this project good luck 2021-12-30 17:38:10   \n",
       "6                     This account is worth a follow 2021-12-30 17:41:02   \n",
       "\n",
       "       user_joined_dt  \n",
       "0 2021-08-21 13:39:34  \n",
       "1 2019-03-20 14:31:32  \n",
       "2 2015-04-29 21:00:18  \n",
       "3 2009-07-10 15:05:09  \n",
       "4 2021-02-23 00:52:38  \n",
       "5 2021-06-05 07:13:05  \n",
       "6 2009-03-22 15:14:18  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Select the columns to be processed, including the text column\n",
    "df_processed = df.select(all_cols_to_process)\n",
    "\n",
    "# Drop duplicates in the text column and remove and rows where the text is a blank string\n",
    "df_processed = df_processed.dropDuplicates(subset=[\"reviewText\"]).filter(df[\"reviewText\"] != '')\n",
    "\n",
    "# Drop rows with a missing value in the text column\n",
    "df_processed = df_processed.na.drop(subset=[\"reviewText\"])\n",
    "\n",
    "# Apply datetime formatting for the two datetime columns\n",
    "for c in [\"created_at\", \"user_joined\"]:\n",
    "    df_processed = df_processed.withColumn(\n",
    "        f\"{c}_dt\",\n",
    "        F.to_timestamp(F.col(c), \"yyyy-MM-dd HH:mm:ss\"),\n",
    "    )\n",
    "\n",
    "print(df_processed.count())\n",
    "show_pyspark_df(df_processed, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07446163",
   "metadata": {},
   "source": [
    "The non-text data processing is now ready and we can proceed to preparing the text data column for quantitative analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61eca04f",
   "metadata": {},
   "source": [
    "### Processing Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b84e31d",
   "metadata": {},
   "source": [
    "We'll now process the text data column. All the processed data columns from the previous section, including the text column, will be retained. However, here, we will only be processing the text column from this data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631ac44b",
   "metadata": {},
   "source": [
    "#### Cleaning Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020cd926",
   "metadata": {},
   "source": [
    "Since we are looking to build up a useful text vocabulary to perform NLP tasks on, we'll first perform the following text cleaning steps\n",
    "- change text to lowercase\n",
    "- remove numbers\n",
    "- remove punctuation\n",
    "- replace multiple whitespaces with a single whitespace\n",
    "- remove leading and trailing whitespace\n",
    "\n",
    "The second and third of these steps will eliminate non-text components of the data in the text (tweet text) column. The last two of these steps will help with [tokenization](https://neptune.ai/blog/tokenization-in-nlp) during the NLP data preparation step (done in the next sub-section)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "07963ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>user_joined</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>reviewText_processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-12-30 17:41:12+00:00</td>\n",
       "      <td>2021-08-21 13:39:34+00:00</td>\n",
       "      <td>I love you NASA</td>\n",
       "      <td>i love you nasa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-12-30 17:48:32+00:00</td>\n",
       "      <td>2019-03-20 14:31:32+00:00</td>\n",
       "      <td>The X on my back is so large NASA can see it...</td>\n",
       "      <td>the x on my back is so large nasa can see it l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-12-30 17:47:42+00:00</td>\n",
       "      <td>2015-04-29 21:00:18+00:00</td>\n",
       "      <td>Moon and Uranus !</td>\n",
       "      <td>moon and uranus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-12-30 17:43:37+00:00</td>\n",
       "      <td>2009-07-10 15:05:09+00:00</td>\n",
       "      <td>\"  Church people be like \"\"we all know who cre...</td>\n",
       "      <td>church people be like we all know who created ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-12-30 17:40:18+00:00</td>\n",
       "      <td>2021-02-23 00:52:38+00:00</td>\n",
       "      <td>Blue Origin is looking to hire a Work Center S...</td>\n",
       "      <td>blue origin is looking to hire a work center s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2021-12-30 17:38:10+00:00</td>\n",
       "      <td>2021-06-05 07:13:05+00:00</td>\n",
       "      <td>I support this project good luck</td>\n",
       "      <td>i support this project good luck</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2021-12-30 17:41:02+00:00</td>\n",
       "      <td>2009-03-22 15:14:18+00:00</td>\n",
       "      <td>This account is worth a follow</td>\n",
       "      <td>this account is worth a follow</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  created_at                user_joined  \\\n",
       "0  2021-12-30 17:41:12+00:00  2021-08-21 13:39:34+00:00   \n",
       "1  2021-12-30 17:48:32+00:00  2019-03-20 14:31:32+00:00   \n",
       "2  2021-12-30 17:47:42+00:00  2015-04-29 21:00:18+00:00   \n",
       "3  2021-12-30 17:43:37+00:00  2009-07-10 15:05:09+00:00   \n",
       "4  2021-12-30 17:40:18+00:00  2021-02-23 00:52:38+00:00   \n",
       "5  2021-12-30 17:38:10+00:00  2021-06-05 07:13:05+00:00   \n",
       "6  2021-12-30 17:41:02+00:00  2009-03-22 15:14:18+00:00   \n",
       "\n",
       "                                          reviewText  \\\n",
       "0                                    I love you NASA   \n",
       "1    The X on my back is so large NASA can see it...   \n",
       "2                                  Moon and Uranus !   \n",
       "3  \"  Church people be like \"\"we all know who cre...   \n",
       "4  Blue Origin is looking to hire a Work Center S...   \n",
       "5                   I support this project good luck   \n",
       "6                     This account is worth a follow   \n",
       "\n",
       "                                reviewText_processed  \n",
       "0                                    i love you nasa  \n",
       "1  the x on my back is so large nasa can see it l...  \n",
       "2                                    moon and uranus  \n",
       "3  church people be like we all know who created ...  \n",
       "4  blue origin is looking to hire a work center s...  \n",
       "5                   i support this project good luck  \n",
       "6                     this account is worth a follow  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Change text to lowercase\n",
    "df_processed = df_processed.select(\n",
    "    all_cols_to_process + [\"created_at_dt\", \"user_joined_dt\"] + [F.lower(F.col(\"reviewText\")).alias(\"reviewText_processed\")]\n",
    ")\n",
    "\n",
    "# Remove numbers\n",
    "df_processed.select(\n",
    "    all_cols_to_process\n",
    "    + [\n",
    "        F.regexp_replace(F.col(\"reviewText_processed\"), \"\\d+\", \"\").alias(\n",
    "            \"reviewText_processed\"\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Remove punctuation\n",
    "df_processed = df_processed.select(\n",
    "    all_cols_to_process\n",
    "    + [remove_punctuation(F.col(\"reviewText_processed\")).alias(\"reviewText_processed\")]\n",
    ")\n",
    "\n",
    "# Replace multiple whitespaces with a single whitespace\n",
    "df_processed = df_processed.select(\n",
    "    all_cols_to_process\n",
    "    + [\n",
    "        replace_multiple_spaces(F.col(\"reviewText_processed\")).alias(\n",
    "            \"reviewText_processed\"\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Remove leading and trailing spaces\n",
    "df_processed = df_processed.select(\n",
    "    all_cols_to_process\n",
    "    + [\n",
    "        remove_lead_trail_spaces(F.col(\"reviewText_processed\")).alias(\n",
    "            \"reviewText_processed\"\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "print(df_processed.count())\n",
    "show_pyspark_df(df_processed, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f0e340",
   "metadata": {},
   "source": [
    "#### NLP on Cleaned Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addcae40",
   "metadata": {},
   "source": [
    "Now, we'll apply an NLP pipeline to extract features from the cleaned text data. This pipeline will consist of the following three steps\n",
    "- tokenization\n",
    "  - here we will restrict the minimum token length that we will accept using the `minTokenLength` key word\n",
    "    - this is a hyperparameter of the NLP pipeline that can be tuned during future versions of this analysis\n",
    "- removal of stop words\n",
    "  - these are frequently occurring words that won't offer any useful information\n",
    "- vectorization\n",
    "  - this is the process of associating words or phrases from a text vocabulary to a real-valued vector\n",
    "  - there are several approaches to vectorization, but we will restrict ourselves to TFIDF vectorization ([1](https://openclassrooms.com/en/courses/6532301-introduction-to-natural-language-processing/7067116-apply-the-tf-idf-vectorization-approach), [2](https://monkeylearn.com/blog/what-is-tf-idf/))\n",
    "    - briefly, the disadvantage of the TFIDF technique is that the same words in two different vocabularies will produce different vector representations depending on the corpus being analysed\n",
    "    - in PySpark this can be done using a combination of a `CountVectorizer` ([link](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.CountVectorizer.html)) and `IDF` ([link](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.IDF.html)) classes from the `pyspark.ml` module\n",
    "      - `CountVectorizer` has three particularly useful hyperparameters `minDF`, `maxDF` and `vocabSize` that could be extensively tuned in future versions of this analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f4f3c65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "tokenizer = RegexTokenizer(\n",
    "    minTokenLength=3,\n",
    "    inputCol=\"reviewText_processed\",\n",
    "    outputCol=\"tokens\",\n",
    "    toLowercase=True,\n",
    "    pattern=\"\\\\s+\",  # default (https://stackoverflow.com/a/13750765/4057186)\n",
    "    # pattern=\"\\\\W\",  # keep words\n",
    ")\n",
    "\n",
    "# Removal of Stop Words\n",
    "remover = StopWordsRemover(\n",
    "    inputCol=\"tokens\", outputCol=\"tokens_no_stopwords\", stopWords=all_stopwords\n",
    ")\n",
    "\n",
    "# TFIDF Vectorization\n",
    "count_vec_params = dict(\n",
    "    inputCol=\"tokens_no_stopwords\",\n",
    "    outputCol=\"rawFeatures\",\n",
    "    vocabSize=262144,  # default = 262144\n",
    "    minDF=5,  # if this is a float, ignores terms with document freq less than this fraction; default = 1.0\n",
    "    maxDF=0.75,  # if this is a float, ignores tokens with a document freq greater than this fraction; default = 9223372036854775807\n",
    ")\n",
    "count_vectorizer = CountVectorizer(**count_vec_params)\n",
    "idf = IDF(minDocFreq=0, inputCol=\"rawFeatures\", outputCol=\"1gram_idfv\")\n",
    "tfidf_vectorizer = Pipeline(stages=[count_vectorizer, idf])\n",
    "\n",
    "assembler = VectorAssembler(inputCols=[\"1gram_idf\"], outputCol=\"features\")\n",
    "\n",
    "# Combined text processing pipeline\n",
    "pipe = Pipeline(stages=[tokenizer, remover, tfidf_vectorizer])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a558cadb",
   "metadata": {},
   "source": [
    "Apply the text processing pipeline to process the text column from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "63789134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1081\n",
      "CPU times: user 208 ms, sys: 20.9 ms, total: 229 ms\n",
      "Wall time: 5.91 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>user_joined</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>reviewText_processed</th>\n",
       "      <th>tokens</th>\n",
       "      <th>tokens_no_stopwords</th>\n",
       "      <th>rawFeatures</th>\n",
       "      <th>features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-12-30 17:41:12+00:00</td>\n",
       "      <td>2021-08-21 13:39:34+00:00</td>\n",
       "      <td>I love you NASA</td>\n",
       "      <td>i love you nasa</td>\n",
       "      <td>[love, you, nasa]</td>\n",
       "      <td>[love, nasa]</td>\n",
       "      <td>(0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(0.0, 0.0, 2.443271677136423, 0.0, 0.0, 0.0, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-12-30 17:48:32+00:00</td>\n",
       "      <td>2019-03-20 14:31:32+00:00</td>\n",
       "      <td>The X on my back is so large NASA can see it...</td>\n",
       "      <td>the x on my back is so large nasa can see it l...</td>\n",
       "      <td>[the, back, large, nasa, can, see, love, you]</td>\n",
       "      <td>[back, large, nasa, see, love]</td>\n",
       "      <td>(0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(0.0, 0.0, 2.443271677136423, 0.0, 0.0, 0.0, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-12-30 17:47:42+00:00</td>\n",
       "      <td>2015-04-29 21:00:18+00:00</td>\n",
       "      <td>Moon and Uranus !</td>\n",
       "      <td>moon and uranus</td>\n",
       "      <td>[moon, and, uranus]</td>\n",
       "      <td>[moon, uranus]</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-12-30 17:43:37+00:00</td>\n",
       "      <td>2009-07-10 15:05:09+00:00</td>\n",
       "      <td>\"  Church people be like \"\"we all know who cre...</td>\n",
       "      <td>church people be like we all know who created ...</td>\n",
       "      <td>[church, people, like, all, know, who, created...</td>\n",
       "      <td>[church, people, like, know, created, universe]</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 2.8756925952331156, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-12-30 17:40:18+00:00</td>\n",
       "      <td>2021-02-23 00:52:38+00:00</td>\n",
       "      <td>Blue Origin is looking to hire a Work Center S...</td>\n",
       "      <td>blue origin is looking to hire a work center s...</td>\n",
       "      <td>[blue, origin, looking, hire, work, center, sc...</td>\n",
       "      <td>[blue, origin, looking, hire, work, center, sc...</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2021-12-30 17:38:10+00:00</td>\n",
       "      <td>2021-06-05 07:13:05+00:00</td>\n",
       "      <td>I support this project good luck</td>\n",
       "      <td>i support this project good luck</td>\n",
       "      <td>[support, this, project, good, luck]</td>\n",
       "      <td>[support, project, good, luck]</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2021-12-30 17:41:02+00:00</td>\n",
       "      <td>2009-03-22 15:14:18+00:00</td>\n",
       "      <td>This account is worth a follow</td>\n",
       "      <td>this account is worth a follow</td>\n",
       "      <td>[this, account, worth, follow]</td>\n",
       "      <td>[account, worth, follow]</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2021-12-30 17:41:31+00:00</td>\n",
       "      <td>2019-09-21 17:15:40+00:00</td>\n",
       "      <td>My better alternative is hanging out with peo...</td>\n",
       "      <td>my better alternative is hanging out with peop...</td>\n",
       "      <td>[better, alternative, hanging, out, with, peop...</td>\n",
       "      <td>[better, alternative, hanging, people, make, f...</td>\n",
       "      <td>(0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(0.0, 2.2860860936140104, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2021-12-30 17:43:45+00:00</td>\n",
       "      <td>2013-01-18 14:40:16+00:00</td>\n",
       "      <td>FACTs.</td>\n",
       "      <td>facts</td>\n",
       "      <td>[facts]</td>\n",
       "      <td>[facts]</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2021-12-30 17:41:08+00:00</td>\n",
       "      <td>2020-07-01 06:16:37+00:00</td>\n",
       "      <td>Very cool - this film is about you and your fa...</td>\n",
       "      <td>very cool this film is about you and your fail...</td>\n",
       "      <td>[very, cool, this, film, about, you, and, your...</td>\n",
       "      <td>[cool, film, failings]</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  created_at                user_joined  \\\n",
       "0  2021-12-30 17:41:12+00:00  2021-08-21 13:39:34+00:00   \n",
       "1  2021-12-30 17:48:32+00:00  2019-03-20 14:31:32+00:00   \n",
       "2  2021-12-30 17:47:42+00:00  2015-04-29 21:00:18+00:00   \n",
       "3  2021-12-30 17:43:37+00:00  2009-07-10 15:05:09+00:00   \n",
       "4  2021-12-30 17:40:18+00:00  2021-02-23 00:52:38+00:00   \n",
       "5  2021-12-30 17:38:10+00:00  2021-06-05 07:13:05+00:00   \n",
       "6  2021-12-30 17:41:02+00:00  2009-03-22 15:14:18+00:00   \n",
       "7  2021-12-30 17:41:31+00:00  2019-09-21 17:15:40+00:00   \n",
       "8  2021-12-30 17:43:45+00:00  2013-01-18 14:40:16+00:00   \n",
       "9  2021-12-30 17:41:08+00:00  2020-07-01 06:16:37+00:00   \n",
       "\n",
       "                                          reviewText  \\\n",
       "0                                    I love you NASA   \n",
       "1    The X on my back is so large NASA can see it...   \n",
       "2                                  Moon and Uranus !   \n",
       "3  \"  Church people be like \"\"we all know who cre...   \n",
       "4  Blue Origin is looking to hire a Work Center S...   \n",
       "5                   I support this project good luck   \n",
       "6                     This account is worth a follow   \n",
       "7   My better alternative is hanging out with peo...   \n",
       "8                                             FACTs.   \n",
       "9  Very cool - this film is about you and your fa...   \n",
       "\n",
       "                                reviewText_processed  \\\n",
       "0                                    i love you nasa   \n",
       "1  the x on my back is so large nasa can see it l...   \n",
       "2                                    moon and uranus   \n",
       "3  church people be like we all know who created ...   \n",
       "4  blue origin is looking to hire a work center s...   \n",
       "5                   i support this project good luck   \n",
       "6                     this account is worth a follow   \n",
       "7  my better alternative is hanging out with peop...   \n",
       "8                                              facts   \n",
       "9  very cool this film is about you and your fail...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0                                  [love, you, nasa]   \n",
       "1      [the, back, large, nasa, can, see, love, you]   \n",
       "2                                [moon, and, uranus]   \n",
       "3  [church, people, like, all, know, who, created...   \n",
       "4  [blue, origin, looking, hire, work, center, sc...   \n",
       "5               [support, this, project, good, luck]   \n",
       "6                     [this, account, worth, follow]   \n",
       "7  [better, alternative, hanging, out, with, peop...   \n",
       "8                                            [facts]   \n",
       "9  [very, cool, this, film, about, you, and, your...   \n",
       "\n",
       "                                 tokens_no_stopwords  \\\n",
       "0                                       [love, nasa]   \n",
       "1                     [back, large, nasa, see, love]   \n",
       "2                                     [moon, uranus]   \n",
       "3    [church, people, like, know, created, universe]   \n",
       "4  [blue, origin, looking, hire, work, center, sc...   \n",
       "5                     [support, project, good, luck]   \n",
       "6                           [account, worth, follow]   \n",
       "7  [better, alternative, hanging, people, make, f...   \n",
       "8                                            [facts]   \n",
       "9                             [cool, film, failings]   \n",
       "\n",
       "                                         rawFeatures  \\\n",
       "0  (0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "1  (0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3  (0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, ...   \n",
       "4  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "5  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "6  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "7  (0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "8  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "9  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "                                            features  \n",
       "0  (0.0, 0.0, 2.443271677136423, 0.0, 0.0, 0.0, 0...  \n",
       "1  (0.0, 0.0, 2.443271677136423, 0.0, 0.0, 0.0, 0...  \n",
       "2  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "3  (0.0, 0.0, 0.0, 0.0, 0.0, 2.8756925952331156, ...  \n",
       "4  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "5  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "6  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "7  (0.0, 2.2860860936140104, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "8  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "9  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "pipe_trained = pipe.fit(df_processed)\n",
    "df_text_processed = pipe_trained.transform(df_processed)\n",
    "print(df_text_processed.count())\n",
    "show_pyspark_df(df_text_processed, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc004eda",
   "metadata": {},
   "source": [
    "## Topic Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a49adc4",
   "metadata": {},
   "source": [
    "### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f343206",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "num_topics = 10\n",
    "lda_params_dict = dict(\n",
    "    featuresCol=\"features\",  # features or tokens_no_stopwords\n",
    "    optimizer=\"online\",\n",
    "    maxIter=3,\n",
    "    k=num_topics,\n",
    "    topicConcentration=None,\n",
    ")\n",
    "lda = LDA(**lda_params_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f57e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# lda_model = lda.fit(df_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f21be4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# df_full = df_text_processed.join(\n",
    "#     df.select([\"id\", \"created_at\", \"user_joined\", \"reviewText\"]),\n",
    "#     on=[\"created_at\", \"user_joined\", \"reviewText\"],\n",
    "#     how=\"left\",\n",
    "# )\n",
    "# print(df_full.count())\n",
    "# show_pyspark_df(df_full, 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
